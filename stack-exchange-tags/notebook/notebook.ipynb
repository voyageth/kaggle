{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on [this comment][1]\n",
    "\n",
    "  [1]: https://www.kaggle.com/c/facebook-recruiting-iii-keyword-extraction/forums/t/6650/share-your-approach?forumMessageId=36434#post36434"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import IPython.display\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "def maybe_pickle(file_name, load_dataset, force=False):\n",
    "    pickle_file_name = \"pickle/\" + file_name + \".pickle\"\n",
    "    import os\n",
    "    if not os.path.exists(\"pickle\"):\n",
    "        os.makedirs(\"pickle\")\n",
    "        \n",
    "    if os.path.exists(file_name) and not force:\n",
    "        # You may override by setting force=True.\n",
    "        print('%s already present - Skipping pickling.' % file_name)\n",
    "    else:\n",
    "        print('Pickling %s.' % file_name)\n",
    "        dataset = load_dataset(None)\n",
    "        try:\n",
    "            with open(file_name, 'wb') as f:\n",
    "                pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "        except Exception as e:\n",
    "            print('Unable to save data to', file_name, ':', e)\n",
    "    \n",
    "    return pickle_file_name\n",
    "\n",
    "def load_data(file_name):\n",
    "    original_file_path = \"../input/\" + file_name + \".csv\"\n",
    "    pickle_file_name = maybe_pickle(file_name, lambda x: pd.read_csv(original_file_path))\n",
    "    \n",
    "    with open(pickle_file_name, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickle/biology.pickle already present - Skipping pickling.\n",
      "pickle/cooking.pickle already present - Skipping pickling.\n",
      "pickle/biology.pickle already present - Skipping pickling.\n",
      "pickle/biology.pickle already present - Skipping pickling.\n",
      "pickle/biology.pickle already present - Skipping pickling.\n",
      "pickle/biology.pickle already present - Skipping pickling.\n",
      "pickle/biology.pickle already present - Skipping pickling.\n",
      "pickle/biology.pickle already present - Skipping pickling.\n"
     ]
    }
   ],
   "source": [
    "biology = load_data(\"biology\")\n",
    "cooking = load_data(\"cooking\")\n",
    "biology = load_data(\"biology\")\n",
    "biology = load_data(\"biology\")\n",
    "biology = load_data(\"biology\")\n",
    "biology = load_data(\"biology\")\n",
    "biology = load_data(\"biology\")\n",
    "biology = load_data(\"biology\")\n",
    "\n",
    "\n",
    "cooking = pd.read_csv(\"../input/cooking.csv\")\n",
    "crypto = pd.read_csv(\"../input/crypto.csv\")\n",
    "diy = pd.read_csv(\"../input/diy.csv\")\n",
    "robotics = pd.read_csv(\"../input/robotics.csv\")\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "travel = pd.read_csv(\"../input/travel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                              title  \\\n",
      "0   1  What is the criticality of the ribosome bindin...   \n",
      "1   2  How is RNAse contamination in RNA based experi...   \n",
      "2   3      Are lymphocyte sizes clustered in two groups?   \n",
      "3   4  How long does antibiotic-dosed LB maintain goo...   \n",
      "4   5        Is exon order always preserved in splicing?   \n",
      "\n",
      "                                             content  \\\n",
      "0  <p>In prokaryotic translation, how critical fo...   \n",
      "1  <p>Does anyone have any suggestions to prevent...   \n",
      "2  <p>Tortora writes in <em>Principles of Anatomy...   \n",
      "3  <p>Various people in our lab will prepare a li...   \n",
      "4  <p>Are there any cases in which the splicing m...   \n",
      "\n",
      "                                                tags  \n",
      "0  ribosome binding-sites translation synthetic-b...  \n",
      "1                                   rna biochemistry  \n",
      "2                 immunology cell-biology hematology  \n",
      "3                                       cell-culture  \n",
      "4            splicing mrna spliceosome introns exons  \n",
      "   id                                              title  \\\n",
      "0   1  what critic ribosom bind site relat start codo...   \n",
      "1   2         how rnase contamin rna base experi prevent   \n",
      "2   3               are lymphocyt size cluster two group   \n",
      "3   4     how long antibioticdos lb maintain good select   \n",
      "4   5                 is exon order alway preserv splice   \n",
      "\n",
      "                                             content  \\\n",
      "0  in prokaryot translat critic effici translat l...   \n",
      "1  doe anyon suggest prevent rnase contamin work ...   \n",
      "2  tortora write principl anatomi physiolog lymph...   \n",
      "3  various peopl lab prepar liter lb add kanamyci...   \n",
      "4  are case splice machineri construct mrna exon ...   \n",
      "\n",
      "                                                tags  \n",
      "0  ribosome binding-sites translation synthetic-b...  \n",
      "1                                   rna biochemistry  \n",
      "2                 immunology cell-biology hematology  \n",
      "3                                       cell-culture  \n",
      "4            splicing mrna spliceosome introns exons  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "punctuation_trans_table = str.maketrans({key: None for key in string.punctuation})\n",
    "html_tag_regex = re.compile('<.*?>')\n",
    "\n",
    "print(biology.head())\n",
    "\n",
    "def cleaning_text(text):\n",
    "    # TODO remove code fragment\n",
    "    # TODO remove url\n",
    "    # TODO convert to lowercase\n",
    "    # TODO add meta features from original text\n",
    "    ## length of the raw text in chars\n",
    "    ## number of code segments\n",
    "    ## number of 'a href' tags\n",
    "    ## number of times 'http' occurs (count urls)\n",
    "    ## number of times 'grater sign' occurs (count html tags)\n",
    "    # TODO add meta features from cleaned text\n",
    "    ## number of words(tokens) in the clean text\n",
    "    ## length of the clean text in chars\n",
    "    # TODO feature scaling(0-1 range) with min-max\n",
    "    \n",
    "    # remove html tags\n",
    "    text = re.sub(html_tag_regex, '', text)\n",
    "    # remove \\r, \\n\n",
    "    text = text.replace('\\n', ' ').replace('\\r', '')\n",
    "    # remove Punctuations\n",
    "    text = text.translate(punctuation_trans_table)\n",
    "    # split\n",
    "    words = word_tokenize(text)\n",
    "    # remove stop words\n",
    "    words = [word for word in words if word not in stopwords.words('english')]\n",
    "    # lemmatizing, stemming\n",
    "    words = [stemmer.stem(wordnet_lemmatizer.lemmatize(word)) for word in words]\n",
    "    # join\n",
    "    text = ' '.join(words)\n",
    "    return text\n",
    "\n",
    "\n",
    "def cleaning(row):\n",
    "    row['title'] = cleaning_text(row['title'])\n",
    "    row['content'] = cleaning_text(row['content'])\n",
    "    return row\n",
    "\n",
    "# TODO remove duplicates\n",
    "# TODO union tags for duplicates\n",
    "cleaned_df = biology.apply(cleaning, axis=1)\n",
    "print(cleaned_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13196\n"
     ]
    }
   ],
   "source": [
    "print(len(cleaned_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 content\n",
      "0      in prokaryot translat critic effici translat l...\n",
      "1      doe anyon suggest prevent rnase contamin work ...\n",
      "2      tortora write principl anatomi physiolog lymph...\n",
      "3      various peopl lab prepar liter lb add kanamyci...\n",
      "4      are case splice machineri construct mrna exon ...\n",
      "5      im interest sequenc analyz bound dna minim amo...\n",
      "6      im look resourc inform format dendrit spine sy...\n",
      "7      i ship 10 Âµl vector miniprep collabor 15 ml ep...\n",
      "8      i notic within exampl experi class differ repo...\n",
      "9      accord endosymbiont theori mitochondrion chlor...\n",
      "10     in highthroughput experi cell cultur treat sta...\n",
      "11     has anyon tri chew backann vitro dna assembl m...\n",
      "12     the whole question what optim frame size secon...\n",
      "13     we know pop scienc psycholog state effect immu...\n",
      "14     there bioinformat consider show signific corre...\n",
      "15     i recent download gene annot homo sapien ensem...\n",
      "16     what strain design flu mean for exampl avian f...\n",
      "17     i know basic epigenet i know epigenet mechan t...\n",
      "18     im tri find good protocol plasmid miniprep im ...\n",
      "19     are cell marin anim flora equip special ion ex...\n",
      "20     im look data mayb chp2 data show chromatin bin...\n",
      "21     assum longer distanc gap myelin sheath benefic...\n",
      "22     we suspect bidirect transcript event happen lo...\n",
      "23     what current consensus whether human receptor ...\n",
      "24     i understand similar structur but easili ident...\n",
      "25     for predict gene sequenc genom need set maximu...\n",
      "26     the t7 polymeras doesnt transcrib sequenc equa...\n",
      "27     the exist nanomet scale microorgan propos use ...\n",
      "28     ive think start small privat research project ...\n",
      "29     onc upon time i chanc upon old microbiolog boo...\n",
      "...                                                  ...\n",
      "13166  my parent alway told eat appl tree near road k...\n",
      "13167  so basic found thing home it super small i zoo...\n",
      "13168  if congenit blind person get eye sight either ...\n",
      "13169  as neuron fire work togeth connect strengthen ...\n",
      "13170  it back winter onion root tip experi colleg we...\n",
      "13171  whi blood group o peopl a b antibodi though do...\n",
      "13172  which organ would process filter blood shortes...\n",
      "13173  ive seen report special herbivor plant build s...\n",
      "13174  some enzym oper kinet faster diffus rate which...\n",
      "13175  i read mani articl bacteriophag like lambda ph...\n",
      "13176  research uncov neuron seem listen specif input...\n",
      "13177  i know thing nucleus rewrit dna protein send n...\n",
      "13178  coffe usual serv temperatur would burn hand sp...\n",
      "13179  im tri figur relat conform entropi protein fol...\n",
      "13180  how human poss plethera genet defect yet ances...\n",
      "13181  the differ diarrhoea dysenteri quit clear appe...\n",
      "13182  i saw someon search googl affirm deni didnt fi...\n",
      "13183  if watch last olymp like probabl also observ m...\n",
      "13184  im pretti new eeg analysi i measur eeg six sca...\n",
      "13185  allow apolog advanc layman terminolog im wonde...\n",
      "13186  the common exampl exogen pyrogen endotoxin apa...\n",
      "13187  the asian tiger snake inject venom bite also e...\n",
      "13188  i consid food product wast manag 3d food print...\n",
      "13189  at least i believ chrysali i found hang gate d...\n",
      "13190  i watch smarter everi day episod hypoxia inter...\n",
      "13191  had sore throat sore mouth the doctor diagnos ...\n",
      "13192  besid fruit milk exampl natur benefici individ...\n",
      "13193  what i understand perman stretch vessel wall i...\n",
      "13194  i want start record concept particular subject...\n",
      "13195  i took photograph tree ring each divis ruler 1...\n",
      "\n",
      "[13196 rows x 1 columns]\n",
      "  (0, 12523)\t0.120612338712\n",
      "  (0, 25023)\t0.157488359995\n",
      "  (0, 6369)\t0.133603543064\n",
      "  (0, 6090)\t0.348485775607\n",
      "  (0, 3445)\t0.32400093874\n",
      "  (0, 33104)\t0.155603571764\n",
      "  (0, 18675)\t0.195261252058\n",
      "  (0, 9339)\t0.195943328919\n",
      "  (0, 32441)\t0.267732578259\n",
      "  (0, 29433)\t0.124191996023\n",
      "  (0, 31584)\t0.148569968154\n",
      "  (0, 6780)\t0.156742573526\n",
      "  (0, 29886)\t0.201563513586\n",
      "  (0, 21244)\t0.161736628034\n",
      "  (0, 12532)\t0.173535276741\n",
      "  (0, 10235)\t0.216224552536\n",
      "  (0, 34574)\t0.545545939178\n",
      "  (0, 28049)\t0.201563513586\n",
      "  (1, 34252)\t0.220712540775\n",
      "  (1, 27063)\t0.266245082469\n",
      "  (1, 14152)\t0.210234920737\n",
      "  (1, 36362)\t0.146729302443\n",
      "  (1, 14817)\t0.181685382537\n",
      "  (1, 34630)\t0.198896388388\n",
      "  (1, 11134)\t0.318954513452\n",
      "  :\t:\n",
      "  (13195, 22807)\t0.141562555416\n",
      "  (13195, 30835)\t0.223224543698\n",
      "  (13195, 36678)\t0.122803495835\n",
      "  (13195, 7344)\t0.113705734928\n",
      "  (13195, 14054)\t0.122182509878\n",
      "  (13195, 29537)\t0.0930735408987\n",
      "  (13195, 23854)\t0.111943840168\n",
      "  (13195, 34368)\t0.0979120197791\n",
      "  (13195, 26845)\t0.115670570417\n",
      "  (13195, 10714)\t0.0974760327265\n",
      "  (13195, 16157)\t0.0747259525996\n",
      "  (13195, 16164)\t0.08262312955\n",
      "  (13195, 16087)\t0.0971914389405\n",
      "  (13195, 37204)\t0.142427245395\n",
      "  (13195, 34643)\t0.244383302654\n",
      "  (13195, 11745)\t0.098511212202\n",
      "  (13195, 6303)\t0.398452889663\n",
      "  (13195, 29943)\t0.570405149286\n",
      "  (13195, 27110)\t0.0727274048947\n",
      "  (13195, 31151)\t0.0786692102289\n",
      "  (13195, 5602)\t0.0773391912764\n",
      "  (13195, 15894)\t0.071587547827\n",
      "  (13195, 28769)\t0.0445852463848\n",
      "  (13195, 21736)\t0.0550696071236\n",
      "  (13195, 31738)\t0.143238162087\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "stop_words = text.ENGLISH_STOP_WORDS\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "print(cleaned_df[['content']])\n",
    "trans_word_data = vectorizer.fit_transform(cleaned_df['content'].tolist())\n",
    "print(trans_word_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cleaned_df' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2b52676988cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# TODO extract most common tags\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtags_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcleaned_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tags'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtotal_tags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtags_list\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_tags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_tags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cleaned_df' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# TODO extract most common tags\n",
    "tags_list = cleaned_df['tags'].str.split(pat=' ').tolist()\n",
    "total_tags = pd.Series([item for sublist in tags_list for item in sublist])\n",
    "print(len(total_tags))\n",
    "print(total_tags.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO train SGD classifier with one-vs-rest approach\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(loss=\"modified_huber\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python35]",
   "language": "python",
   "name": "conda-env-python35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}