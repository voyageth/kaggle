{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on [this comment][1]\n",
    "\n",
    "  [1]: https://www.kaggle.com/c/facebook-recruiting-iii-keyword-extraction/forums/t/6650/share-your-approach?forumMessageId=36434#post36434"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import IPython.display\n",
    "from six.moves import cPickle as pickle\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "def maybe_pickle(file_name, load_dataset, force=False):\n",
    "    pickle_file_name = \"pickle/\" + file_name + \".pickle\"\n",
    "    import os\n",
    "    if not os.path.exists(\"pickle\"):\n",
    "        os.makedirs(\"pickle\")\n",
    "        \n",
    "    if os.path.exists(pickle_file_name) and not force:\n",
    "        # You may override by setting force=True.\n",
    "        print('%s already present - Skipping pickling.' % pickle_file_name)\n",
    "    else:\n",
    "        print('Pickling %s.' % pickle_file_name)\n",
    "        dataset = load_dataset(None)\n",
    "        try:\n",
    "            with open(pickle_file_name, 'wb') as f:\n",
    "                pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "        except Exception as e:\n",
    "            print('Unable to save data to', file_name, ':', e)\n",
    "    \n",
    "    return pickle_file_name\n",
    "\n",
    "def load_data(file_name, force=False):\n",
    "    original_file_path = \"../input/\" + file_name + \".csv\"\n",
    "    pickle_file_name = maybe_pickle(file_name, lambda x: pd.read_csv(original_file_path), force)\n",
    "    \n",
    "    with open(pickle_file_name, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickle/biology.pickle already present - Skipping pickling.\n",
      "pickle/cooking.pickle already present - Skipping pickling.\n",
      "pickle/crypto.pickle already present - Skipping pickling.\n",
      "pickle/diy.pickle already present - Skipping pickling.\n",
      "pickle/robotics.pickle already present - Skipping pickling.\n",
      "pickle/travel.pickle already present - Skipping pickling.\n"
     ]
    }
   ],
   "source": [
    "biology = load_data(\"biology\")\n",
    "cooking = load_data(\"cooking\")\n",
    "crypto = load_data(\"crypto\")\n",
    "diy = load_data(\"diy\")\n",
    "robotics = load_data(\"robotics\")\n",
    "travel = load_data(\"travel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "punctuation_trans_table = str.maketrans({key: None for key in string.punctuation})\n",
    "html_tag_regex = re.compile('<.*?>')\n",
    "\n",
    "def cleaning_text(text):\n",
    "    # TODO remove code fragment\n",
    "    # TODO remove url\n",
    "    # TODO convert to lowercase\n",
    "    # TODO add meta features from original text\n",
    "    ## length of the raw text in chars\n",
    "    ## number of code segments\n",
    "    ## number of 'a href' tags\n",
    "    ## number of times 'http' occurs (count urls)\n",
    "    ## number of times 'grater sign' occurs (count html tags)\n",
    "    # TODO add meta features from cleaned text\n",
    "    ## number of words(tokens) in the clean text\n",
    "    ## length of the clean text in chars\n",
    "    # TODO feature scaling(0-1 range) with min-max\n",
    "    \n",
    "    # remove html tags\n",
    "    text = re.sub(html_tag_regex, '', text)\n",
    "    # remove \\r, \\n\n",
    "    text = text.replace('\\n', ' ').replace('\\r', '')\n",
    "    # remove Punctuations\n",
    "    text = text.translate(punctuation_trans_table)\n",
    "    # split\n",
    "    words = word_tokenize(text)\n",
    "    # remove stop words\n",
    "    words = [word for word in words if word not in stopwords.words('english')]\n",
    "    # lemmatizing, stemming\n",
    "    words = [wordnet_lemmatizer.lemmatize(word) for word in words]\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    # join\n",
    "    text = ' '.join(words)\n",
    "    return text\n",
    "\n",
    "\n",
    "def cleaning(row):\n",
    "    row['title'] = cleaning_text(row['title'])\n",
    "    row['content'] = cleaning_text(row['content'])\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_cleaned_df(file_name, force=False):\n",
    "    original_file_path = \"../input/\" + file_name + \".csv\"\n",
    "    df = pd.read_csv(original_file_path)\n",
    "    print(\"total len : %d\" % len(df))\n",
    "    return df.progress_apply(cleaning, axis=1)\n",
    "    \n",
    "def maybe_pickle_cleaned_df(file_name, force=False):\n",
    "    pickle_file_name = maybe_pickle(file_name + \"_cleaned\", lambda x: load_cleaned_df(file_name), force)\n",
    "    \n",
    "    with open(pickle_file_name, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickle/biology_cleaned.pickle already present - Skipping pickling.\n",
      "pickle/cooking_cleaned.pickle already present - Skipping pickling.\n",
      "pickle/crypto_cleaned.pickle already present - Skipping pickling.\n",
      "pickle/diy_cleaned.pickle already present - Skipping pickling.\n",
      "pickle/robotics_cleaned.pickle already present - Skipping pickling.\n",
      "pickle/travel_cleaned.pickle already present - Skipping pickling.\n"
     ]
    }
   ],
   "source": [
    "biology_cleaned_df = maybe_pickle_cleaned_df('biology')\n",
    "cooking_cleaned_df = maybe_pickle_cleaned_df('cooking')\n",
    "crypto_cleaned_df = maybe_pickle_cleaned_df('crypto')\n",
    "diy_cleaned_df = maybe_pickle_cleaned_df('diy')\n",
    "robotics_cleaned_df = maybe_pickle_cleaned_df('robotics')\n",
    "travel_cleaned_df = maybe_pickle_cleaned_df('travel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 12523)\t0.120612338712\n",
      "  (0, 25023)\t0.157488359995\n",
      "  (0, 6369)\t0.133603543064\n",
      "  (0, 6090)\t0.348485775607\n",
      "  (0, 3445)\t0.32400093874\n",
      "  (0, 33104)\t0.155603571764\n",
      "  (0, 18675)\t0.195261252058\n",
      "  (0, 9339)\t0.195943328919\n",
      "  (0, 32441)\t0.267732578259\n",
      "  (0, 29433)\t0.124191996023\n",
      "  (0, 31584)\t0.148569968154\n",
      "  (0, 6780)\t0.156742573526\n",
      "  (0, 29886)\t0.201563513586\n",
      "  (0, 21244)\t0.161736628034\n",
      "  (0, 12532)\t0.173535276741\n",
      "  (0, 10235)\t0.216224552536\n",
      "  (0, 34574)\t0.545545939178\n",
      "  (0, 28049)\t0.201563513586\n",
      "  (1, 34252)\t0.220712540775\n",
      "  (1, 27063)\t0.266245082469\n",
      "  (1, 14152)\t0.210234920737\n",
      "  (1, 36362)\t0.146729302443\n",
      "  (1, 14817)\t0.181685382537\n",
      "  (1, 34630)\t0.198896388388\n",
      "  (1, 11134)\t0.318954513452\n",
      "  :\t:\n",
      "  (13195, 22807)\t0.141562555416\n",
      "  (13195, 30835)\t0.223224543698\n",
      "  (13195, 36678)\t0.122803495835\n",
      "  (13195, 7344)\t0.113705734928\n",
      "  (13195, 14054)\t0.122182509878\n",
      "  (13195, 29537)\t0.0930735408987\n",
      "  (13195, 23854)\t0.111943840168\n",
      "  (13195, 34368)\t0.0979120197791\n",
      "  (13195, 26845)\t0.115670570417\n",
      "  (13195, 10714)\t0.0974760327265\n",
      "  (13195, 16157)\t0.0747259525996\n",
      "  (13195, 16164)\t0.08262312955\n",
      "  (13195, 16087)\t0.0971914389405\n",
      "  (13195, 37204)\t0.142427245395\n",
      "  (13195, 34643)\t0.244383302654\n",
      "  (13195, 11745)\t0.098511212202\n",
      "  (13195, 6303)\t0.398452889663\n",
      "  (13195, 29943)\t0.570405149286\n",
      "  (13195, 27110)\t0.0727274048947\n",
      "  (13195, 31151)\t0.0786692102289\n",
      "  (13195, 5602)\t0.0773391912764\n",
      "  (13195, 15894)\t0.071587547827\n",
      "  (13195, 28769)\t0.0445852463848\n",
      "  (13195, 21736)\t0.0550696071236\n",
      "  (13195, 31738)\t0.143238162087\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "stop_words = text.ENGLISH_STOP_WORDS\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "biology_vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "biology_content_vectors = biology_vectorizer.fit_transform(biology_cleaned_df['content'].tolist())\n",
    "print(biology_content_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cleaned_df' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2b52676988cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# TODO extract most common tags\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtags_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcleaned_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tags'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtotal_tags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtags_list\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_tags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_tags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cleaned_df' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# TODO extract most common tags\n",
    "tags_list = cleaned_df['tags'].str.split(pat=' ').tolist()\n",
    "total_tags = pd.Series([item for sublist in tags_list for item in sublist])\n",
    "print(len(total_tags))\n",
    "print(total_tags.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO train SGD classifier with one-vs-rest approach\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(loss=\"modified_huber\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python35]",
   "language": "python",
   "name": "conda-env-python35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}