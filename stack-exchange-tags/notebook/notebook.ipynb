{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on [this comment][1]\n",
    "\n",
    "  [1]: https://www.kaggle.com/c/facebook-recruiting-iii-keyword-extraction/forums/t/6650/share-your-approach?forumMessageId=36434#post36434"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "biology = pd.read_csv(\"../input/biology.csv\")\n",
    "cooking = pd.read_csv(\"../input/cooking.csv\")\n",
    "crypto = pd.read_csv(\"../input/crypto.csv\")\n",
    "diy = pd.read_csv(\"../input/diy.csv\")\n",
    "robotics = pd.read_csv(\"../input/robotics.csv\")\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "travel = pd.read_csv(\"../input/travel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "punctuation_trans_table = str.maketrans({key: None for key in string.punctuation})\n",
    "html_tag_regex = re.compile('<.*?>')\n",
    "\n",
    "print(biology.head())\n",
    "\n",
    "def cleaning_text(text):\n",
    "    # TODO remove code fragment\n",
    "    # TODO remove url\n",
    "    # TODO convert to lowercase\n",
    "    # TODO add meta features from original text\n",
    "    ## length of the raw text in chars\n",
    "    ## number of code segments\n",
    "    ## number of 'a href' tags\n",
    "    ## number of times 'http' occurs (count urls)\n",
    "    ## number of times 'grater sign' occurs (count html tags)\n",
    "    # TODO add meta features from cleaned text\n",
    "    ## number of words(tokens) in the clean text\n",
    "    ## length of the clean text in chars\n",
    "    # TODO feature scaling(0-1 range) with min-max\n",
    "    \n",
    "    # remove html tags\n",
    "    text = re.sub(html_tag_regex, '', text)\n",
    "    # remove \\r, \\n\n",
    "    text = text.replace('\\n', ' ').replace('\\r', '')\n",
    "    # remove Punctuations\n",
    "    text = text.translate(punctuation_trans_table)\n",
    "    # split\n",
    "    words = word_tokenize(text)\n",
    "    # remove stop words\n",
    "    words = [word for word in words if word not in stopwords.words('english')]\n",
    "    # lemmatizing, stemming\n",
    "    words = [stemmer.stem(wordnet_lemmatizer.lemmatize(word)) for word in words]\n",
    "    # join\n",
    "    text = ' '.join(words)\n",
    "    return text\n",
    "\n",
    "\n",
    "def cleaning(row):\n",
    "    row['title'] = cleaning_text(row['title'])\n",
    "    row['content'] = cleaning_text(row['content'])\n",
    "    return row\n",
    "\n",
    "# TODO remove duplicates\n",
    "# TODO union tags for duplicates\n",
    "cleaned_df = biology.apply(cleaning, axis=1)\n",
    "print(cleaned_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(cleaned_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "stop_words = text.ENGLISH_STOP_WORDS\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "print(cleaned_df[['content']])\n",
    "trans_word_data = vectorizer.fit_transform(cleaned_df['content'].tolist())\n",
    "print(trans_word_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO extract most common tags\n",
    "tags_list = cleaned_df['tags'].str.split(pat=' ').tolist()\n",
    "total_tags = pd.Series([item for sublist in tags_list for item in sublist])\n",
    "print(len(total_tags))\n",
    "print(total_tags.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO train SGD classifier with one-vs-rest approach\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(loss=\"modified_huber\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
