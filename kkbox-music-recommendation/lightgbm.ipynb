{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 0.66869 : initial\n",
    "- 0.67947 : change kflod 3 -> 10, early stapping 10, num_round 1000, num_leaves 256\n",
    "- 0.68066 : drop wrong expiretime user's data, add register time/expire time year/month/day\n",
    "- 0.68041 : 'learning_rate': 0.3,'min_data_in_leaf':256,'num_leaves': 512,'max_bin': 256,'max_depth': 20,\n",
    "- 0.67679 : add cf result 'learning_rate': 0.1,'min_data_in_leaf':512,'num_leaves': 512,'max_bin': 512,'max_depth': 20,\n",
    "- 0.67682 : 'learning_rate': 0.3,'min_data_in_leaf':256,'num_leaves': 256,'max_bin': 256,'max_depth': 20,\n",
    "- 0.67758 : 'learning_rate': 0.3,'num_leaves': 256,'max_bin': 256,'max_depth': 20,'min_data_in_leaf':default\n",
    "- 0.67314 : add msno, artist_name avg/count/std\n",
    "- 0.67304 : 'learning_rate': 0.1,'num_leaves': 256,'max_bin': 256,'max_depth': 20,\n",
    "- 0.65317 : add song extra info\n",
    "- 0.65288 : 'learning_rate': 0.1,'num_leaves': 256,'max_bin': 256,'max_depth': 20, 'min_data_in_leaf':256,\n",
    "- 0.65764 : 'learning_rate': 0.1,'num_leaves': 2048,'max_bin': 512,'max_depth': 30, 'min_data_in_leaf':256,\n",
    "- 0.64783 : 'learning_rate': 0.1,'num_leaves': 2048,'max_bin': 512,'max_depth': 10, 'min_data_in_leaf':256,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from six.moves import cPickle as pickle\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "INPUT_DATA_PATH = 'input/'\n",
    "\n",
    "def make_pickle(file_name, data, force=False):\n",
    "    import os\n",
    "    if not os.path.exists(\"pickle\"):\n",
    "        os.makedirs(\"pickle\")\n",
    "        \n",
    "    if os.path.exists(file_name) and not force:\n",
    "        # You may override by setting force=True.\n",
    "        print('%s already present - Skipping pickling.' % file_name)\n",
    "    else:\n",
    "        print('Pickling %s.' % file_name)\n",
    "        try:\n",
    "            with open(file_name, 'wb') as f:\n",
    "                pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "        except Exception as e:\n",
    "            print('Unable to save data to', file_name, ':', e)\n",
    "    \n",
    "    return file_name\n",
    "\n",
    "# draw numeric column plot\n",
    "def draw_scatter_plot(df, col_name):\n",
    "    np_array = df[col_name].values\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(range(len(np_array)), np.sort(np_array))\n",
    "    plt.xlabel('index', fontsize=12)\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.ylabel(col_name, fontsize=12)\n",
    "    plt.show()\n",
    "    \n",
    "def draw_dist_plot(df, col_name):\n",
    "    np_array = df[col_name].values\n",
    "    plt.figure(figsize=(12,8))\n",
    "    sns.distplot(np_array, bins=50, kde=False)\n",
    "    plt.xlabel(col_name, fontsize=12)\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.ylabel('count', fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "def draw_np_array_scatter_plot(np_array, col_name):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(range(len(np_array)), np.sort(np_array))\n",
    "    plt.xlabel('index', fontsize=12)\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.ylabel(col_name, fontsize=12)\n",
    "    plt.show()\n",
    "    \n",
    "def draw_np_array_dist_plot(np_array, col_name):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    sns.distplot(np_array, bins=50, kde=False)\n",
    "    plt.xlabel(col_name, fontsize=12)\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.ylabel('count', fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "# draw category column plot\n",
    "def draw_category_col(df, col):\n",
    "    print('null count : {}'.format(df[col].isnull().sum()))\n",
    "    display(df[col].value_counts())\n",
    "    draw_count_plot(df, col)\n",
    "    draw_bar_plot(df, col, 'target')\n",
    "    draw_factor_count_plot(df, col, \"target\")\n",
    "\n",
    "def draw_count_plot(df, col_name, title='plot'):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    sns.countplot(data=df, x=col_name)\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.xlabel(col_name, fontsize=12)\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.ylabel('count', fontsize=12)\n",
    "    plt.title(title, fontsize=15)\n",
    "    plt.show()\n",
    "    \n",
    "def draw_box_plot(df, x_col, y_col):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    sns.boxplot(data=df, x=x_col, y=y_col)\n",
    "    plt.xlabel(x_col, fontsize=12)\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.ylabel(y_col, fontsize=12)\n",
    "    plt.show()\n",
    "    \n",
    "def draw_violin_plot(df, x_col, y_col):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    sns.violinplot(data=df, x=x_col, y=y_col)\n",
    "    plt.xlabel(x_col, fontsize=12)\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.ylabel(y_col, fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "def draw_factor_count_plot(df, x_col, y_col):\n",
    "    g = sns.factorplot(y_col, col=x_col, data=df, size=3, \n",
    "                       palette=\"muted\", kind='count', col_wrap=4, aspect=.8)\n",
    "    g.despine(left=True)\n",
    "    g.set_ylabels(y_col)\n",
    "    g.set_titles(\"{col_name}\")\n",
    "    g.set_xlabels(\"\")\n",
    "    plt.xticks(rotation='vertical')\n",
    "\n",
    "def draw_bar_plot(df, x_col, y_col):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    g = sns.barplot(x=x_col, y=y_col, data=df, palette=\"muted\")\n",
    "    plt.xlabel(x_col, fontsize=12)\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.ylabel(y_col, fontsize=12)\n",
    "\n",
    "# etc\n",
    "def category_to_numeric(df, column_name):\n",
    "    for category in df[column_name].unique():\n",
    "        category_column = column_name + '_' + str(category)\n",
    "        if category_column in df.columns:\n",
    "            df = df.drop(category_column, axis=1)\n",
    "    df= pd.concat([df,pd.get_dummies(df[column_name], prefix=column_name)],axis=1)\n",
    "    return df\n",
    "\n",
    "def convert_outlier_value(df, col_name, upper_percentile=99.0, lower_percentile=1.0):\n",
    "    np_array = df[col_name].values\n",
    "    \n",
    "    ulimit = np.percentile(np_array, upper_percentile)\n",
    "    llimit = np.percentile(np_array, lower_percentile)\n",
    "    print('upper limit :', ulimit, ', lower limit :', llimit)\n",
    "    \n",
    "    # convert\n",
    "    df[col_name].loc[df[col_name] > ulimit] = ulimit\n",
    "    df[col_name].loc[df[col_name] < llimit] = llimit\n",
    "\n",
    "# save param\n",
    "def save_obj(obj, datetime_key):\n",
    "    with open('lightgbm/'+ datetime_key + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(datetime_key):\n",
    "    with open('lightgbm/' + datetime_key + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling pickle/df_train.\n",
      "Pickling pickle/df_test.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'pickle/df_test'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_pickle('pickle/df_train', df_train, force=True)\n",
    "make_pickle('pickle/df_test', df_test, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('pickle/df_train', 'rb') as f:\n",
    "    df_train = pickle.load(f)\n",
    "with open('pickle/df_test', 'rb') as f:\n",
    "    df_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "msno                            category\n",
       "song_id                         category\n",
       "source_system_tab               category\n",
       "source_screen_name              category\n",
       "source_type                     category\n",
       "target                             uint8\n",
       "city                            category\n",
       "bd                                 uint8\n",
       "gender                          category\n",
       "registered_via                  category\n",
       "membership_days                    int64\n",
       "registration_init_time_year        int64\n",
       "registration_init_time_month       int64\n",
       "registration_init_time_day         int64\n",
       "expiration_date_year               int64\n",
       "expiration_date_month              int64\n",
       "expiration_date_day                int64\n",
       "song_length                       uint32\n",
       "genre_ids                       category\n",
       "artist_name                     category\n",
       "composer                        category\n",
       "lyricist                        category\n",
       "language                        category\n",
       "artist_name_count                float64\n",
       "artist_name_avg                  float64\n",
       "artist_name_std                  float64\n",
       "msno_count                       float64\n",
       "msno_avg                         float64\n",
       "msno_std                         float64\n",
       "isrc_cc                         category\n",
       "isrc_xxx                        category\n",
       "isrc_yyyy                        float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['msno', 'song_id', 'source_system_tab', 'source_screen_name', 'source_type', 'target', 'city', 'bd', 'gender', 'registered_via', 'membership_days', 'song_length', 'genre_ids', 'artist_name', 'composer', 'lyricist', 'language']\n"
     ]
    }
   ],
   "source": [
    "columns = list(df_train.columns)\n",
    "\n",
    "\n",
    "columns.remove('registration_init_time_year')\n",
    "columns.remove('registration_init_time_month')\n",
    "columns.remove('registration_init_time_day')\n",
    "columns.remove('expiration_date_year')\n",
    "columns.remove('expiration_date_month')\n",
    "columns.remove('expiration_date_day')\n",
    "columns.remove('artist_name_count')\n",
    "columns.remove('artist_name_avg')\n",
    "columns.remove('artist_name_std')\n",
    "columns.remove('msno_count')\n",
    "columns.remove('msno_avg')\n",
    "columns.remove('msno_std')\n",
    "columns.remove('isrc_cc')\n",
    "columns.remove('isrc_xxx')\n",
    "columns.remove('isrc_yyyy')\n",
    "\n",
    "test_columns = columns.copy()\n",
    "test_columns.remove('target')\n",
    "\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/voyageth/develop/anaconda3/envs/kaggle/lib/python3.6/site-packages/lightgbm/engine.py:98: UserWarning: Found `num_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds.\n",
      "[10]\tvalid_0's auc: 0.777594\n",
      "[20]\tvalid_0's auc: 0.785565\n",
      "[30]\tvalid_0's auc: 0.789705\n",
      "[40]\tvalid_0's auc: 0.792352\n",
      "[50]\tvalid_0's auc: 0.793547\n",
      "[60]\tvalid_0's auc: 0.794828\n",
      "[70]\tvalid_0's auc: 0.795411\n",
      "[80]\tvalid_0's auc: 0.795808\n",
      "[90]\tvalid_0's auc: 0.796211\n",
      "[100]\tvalid_0's auc: 0.796843\n",
      "[110]\tvalid_0's auc: 0.796903\n",
      "[120]\tvalid_0's auc: 0.798007\n",
      "[130]\tvalid_0's auc: 0.798114\n",
      "[140]\tvalid_0's auc: 0.798199\n",
      "[150]\tvalid_0's auc: 0.798476\n",
      "[160]\tvalid_0's auc: 0.798639\n",
      "Early stopping, best iteration is:\n",
      "[157]\tvalid_0's auc: 0.798695\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[10]\tvalid_0's auc: 0.763357\n",
      "[20]\tvalid_0's auc: 0.772586\n",
      "[30]\tvalid_0's auc: 0.776801\n",
      "[40]\tvalid_0's auc: 0.77944\n",
      "[50]\tvalid_0's auc: 0.780882\n",
      "[60]\tvalid_0's auc: 0.782442\n",
      "[70]\tvalid_0's auc: 0.783128\n",
      "[80]\tvalid_0's auc: 0.783847\n",
      "[90]\tvalid_0's auc: 0.784834\n",
      "[100]\tvalid_0's auc: 0.785007\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[10]\tvalid_0's auc: 0.739463\n",
      "[20]\tvalid_0's auc: 0.74717\n",
      "[30]\tvalid_0's auc: 0.752606\n",
      "[40]\tvalid_0's auc: 0.756233\n",
      "[50]\tvalid_0's auc: 0.757509\n",
      "[60]\tvalid_0's auc: 0.758749\n",
      "[70]\tvalid_0's auc: 0.759613\n",
      "[80]\tvalid_0's auc: 0.760806\n",
      "[90]\tvalid_0's auc: 0.761853\n",
      "[100]\tvalid_0's auc: 0.762504\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[10]\tvalid_0's auc: 0.724063\n",
      "[20]\tvalid_0's auc: 0.731736\n",
      "[30]\tvalid_0's auc: 0.735603\n",
      "[40]\tvalid_0's auc: 0.738383\n",
      "[50]\tvalid_0's auc: 0.73995\n",
      "[60]\tvalid_0's auc: 0.740972\n",
      "[70]\tvalid_0's auc: 0.742015\n",
      "[80]\tvalid_0's auc: 0.743394\n",
      "[90]\tvalid_0's auc: 0.744202\n",
      "[100]\tvalid_0's auc: 0.744563\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[10]\tvalid_0's auc: 0.7103\n",
      "[20]\tvalid_0's auc: 0.718293\n",
      "[30]\tvalid_0's auc: 0.722128\n",
      "[40]\tvalid_0's auc: 0.724564\n",
      "[50]\tvalid_0's auc: 0.726415\n",
      "[60]\tvalid_0's auc: 0.728589\n",
      "[70]\tvalid_0's auc: 0.729607\n",
      "[80]\tvalid_0's auc: 0.73081\n",
      "[90]\tvalid_0's auc: 0.73144\n",
      "[100]\tvalid_0's auc: 0.732322\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[10]\tvalid_0's auc: 0.696564\n",
      "[20]\tvalid_0's auc: 0.70373\n",
      "[30]\tvalid_0's auc: 0.708036\n",
      "[40]\tvalid_0's auc: 0.710588\n",
      "[50]\tvalid_0's auc: 0.712451\n",
      "[60]\tvalid_0's auc: 0.714064\n",
      "[70]\tvalid_0's auc: 0.715029\n",
      "[80]\tvalid_0's auc: 0.715849\n",
      "[90]\tvalid_0's auc: 0.716853\n",
      "[100]\tvalid_0's auc: 0.717101\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[10]\tvalid_0's auc: 0.687226\n",
      "[20]\tvalid_0's auc: 0.69487\n",
      "[30]\tvalid_0's auc: 0.69844\n",
      "[40]\tvalid_0's auc: 0.701612\n",
      "[50]\tvalid_0's auc: 0.703074\n",
      "[60]\tvalid_0's auc: 0.7048\n",
      "[70]\tvalid_0's auc: 0.706297\n",
      "[80]\tvalid_0's auc: 0.707301\n",
      "[90]\tvalid_0's auc: 0.708178\n",
      "[100]\tvalid_0's auc: 0.709021\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[10]\tvalid_0's auc: 0.6795\n",
      "[20]\tvalid_0's auc: 0.686165\n",
      "[30]\tvalid_0's auc: 0.690141\n",
      "[40]\tvalid_0's auc: 0.692694\n",
      "[50]\tvalid_0's auc: 0.694364\n",
      "[60]\tvalid_0's auc: 0.696147\n",
      "[70]\tvalid_0's auc: 0.697549\n",
      "[80]\tvalid_0's auc: 0.698195\n",
      "[90]\tvalid_0's auc: 0.699649\n",
      "[100]\tvalid_0's auc: 0.700057\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[10]\tvalid_0's auc: 0.674164\n",
      "[20]\tvalid_0's auc: 0.680915\n",
      "[30]\tvalid_0's auc: 0.685656\n",
      "[40]\tvalid_0's auc: 0.68843\n",
      "[50]\tvalid_0's auc: 0.689879\n",
      "[60]\tvalid_0's auc: 0.691809\n",
      "[70]\tvalid_0's auc: 0.692801\n",
      "[80]\tvalid_0's auc: 0.69372\n",
      "[90]\tvalid_0's auc: 0.695156\n",
      "[100]\tvalid_0's auc: 0.695689\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[10]\tvalid_0's auc: 0.661388\n",
      "[20]\tvalid_0's auc: 0.667599\n",
      "[30]\tvalid_0's auc: 0.670661\n",
      "[40]\tvalid_0's auc: 0.673039\n",
      "[50]\tvalid_0's auc: 0.674294\n",
      "[60]\tvalid_0's auc: 0.67586\n",
      "[70]\tvalid_0's auc: 0.676789\n",
      "[80]\tvalid_0's auc: 0.677549\n",
      "[90]\tvalid_0's auc: 0.678221\n",
      "[100]\tvalid_0's auc: 0.678934\n",
      "Training process finished. Generating Output...\n",
      "Output created.\n",
      "param saved\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "\n",
    "d_train = df_train[columns]\n",
    "d_test = df_test[test_columns]\n",
    "\n",
    "# Create a Cross Validation with n splits\n",
    "n_splits = 10\n",
    "kf = KFold(n_splits=n_splits)\n",
    "\n",
    "# This array will store the predictions made.\n",
    "predictions = np.zeros(shape=[len(d_test)])\n",
    "\n",
    "import datetime\n",
    "datetime_key = datetime.datetime.now().strftime('%Y-%m-%d_%H:%M:%S')\n",
    "datetime_key = \"[{}]_{}\".format(n_splits, datetime_key)\n",
    "\n",
    "# Create the parameters for LGBM\n",
    "# 'min_data_in_leaf':256,\n",
    "# params = {\n",
    "#     'verbose': 1,\n",
    "#     'objective': 'binary',\n",
    "#     'metric' : 'auc',\n",
    "#     'boosting': 'gbdt',\n",
    "#     'learning_rate': 0.1,\n",
    "#     'num_leaves': 2048,\n",
    "#     'max_bin': 1024,\n",
    "#     'max_depth': 20,\n",
    "#     'bagging_fraction': 0.95,\n",
    "#     'bagging_freq': 1,\n",
    "#     'bagging_seed': 1,\n",
    "#     'feature_fraction': 0.9,\n",
    "#     'feature_fraction_seed': 1,\n",
    "#     'num_rounds': 1000,\n",
    "#     'num_threads' : 8,\n",
    "#     } \n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting': 'gbdt',\n",
    "\n",
    "    'learning_rate': 0.3,'min_data_in_leaf':256,'num_leaves': 1024,'max_bin': 256,'max_depth': 20,\n",
    "    \n",
    "    'verbose': 0,\n",
    "    'bagging_fraction': 0.95,\n",
    "    'bagging_freq': 1,\n",
    "    'bagging_seed': 1,\n",
    "    'feature_fraction': 0.9,\n",
    "    'feature_fraction_seed': 1,\n",
    "    'num_rounds': 1000,\n",
    "    'num_threads' : 8,\n",
    "    'metric' : 'auc',\n",
    "    } \n",
    "\n",
    "# For each KFold\n",
    "for train_indices ,validate_indices in kf.split(d_train) : \n",
    "    train_data = lgb.Dataset(d_train.drop(['target'],axis=1).loc[train_indices,:],\n",
    "                             label=d_train.loc[train_indices,'target'])\n",
    "    val_data = lgb.Dataset(d_train.drop(['target'],axis=1).loc[validate_indices,:],\n",
    "                           label=d_train.loc[validate_indices,'target'])\n",
    "    \n",
    "    # Train the model\n",
    "    bst = lgb.train(params, train_data, valid_sets=[val_data],\n",
    "                   early_stopping_rounds=10, verbose_eval=10)\n",
    "    \n",
    "    # Make the predictions storing them on the predictions array\n",
    "    predictions += bst.predict(d_test)\n",
    "    \n",
    "    # draw feature importance\n",
    "#     lgb.plot_importance(bst)\n",
    "#     plt.show()\n",
    "    \n",
    "    # Release the model from memory for the next iteration\n",
    "    del bst\n",
    "    del train_data\n",
    "    del val_data\n",
    "    gc.collect()\n",
    "\n",
    "print('Training process finished. Generating Output...')\n",
    "\n",
    "# We get the ammount of predictions from the prediction list, by dividing the predictions by the number of Kfolds.\n",
    "predictions = predictions/n_splits\n",
    "\n",
    "# Read the sample_submission CSV\n",
    "submission = pd.read_csv(INPUT_DATA_PATH + '/sample_submission.csv')\n",
    "# Set the target to our predictions\n",
    "submission.target=predictions\n",
    "# Save the submission file\n",
    "submission.to_csv('lightgbm/{}_submission.csv'.format(datetime_key),index=False)\n",
    "\n",
    "print('Output created.')\n",
    "\n",
    "save_obj(params, datetime_key + '_params')\n",
    "save_obj(d_train.columns, datetime_key + '_columns')\n",
    "print('param saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
