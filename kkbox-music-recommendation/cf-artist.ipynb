{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SparkContext, SQLContext\n",
    "\n",
    "sc = SparkContext(appName=\"Pi\")\n",
    "sqlCtx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import operator\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from six.moves import cPickle as pickle\n",
    "def make_pickle(file_name, data, force=False):\n",
    "    import os\n",
    "    if not os.path.exists(\"pickle\"):\n",
    "        os.makedirs(\"pickle\")\n",
    "        \n",
    "    if os.path.exists(file_name) and not force:\n",
    "        # You may override by setting force=True.\n",
    "        print('%s already present - Skipping pickling.' % file_name)\n",
    "    else:\n",
    "        print('Pickling %s.' % file_name)\n",
    "        try:\n",
    "            with open(file_name, 'wb') as f:\n",
    "                pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "        except Exception as e:\n",
    "            print('Unable to save data to', file_name, ':', e)\n",
    "    \n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INPUT_DATA_PATH = 'input/'\n",
    "df_test = pd.read_csv(INPUT_DATA_PATH + 'test.csv',dtype={'msno' : 'category',\n",
    "                                                'source_system_tab' : 'category',\n",
    "                                                'source_screen_name' : 'category',\n",
    "                                                'source_type' : 'category',\n",
    "                                                'song_id' : 'category'})\n",
    "\n",
    "df_train = pd.read_csv(INPUT_DATA_PATH + 'train.csv',dtype={'msno' : 'category',\n",
    "                                                 'source_system_tab' : 'category',\n",
    "                                                  'source_screen_name' : 'category',\n",
    "                                                  'source_type' : 'category',\n",
    "                                                  'target' : np.uint8,\n",
    "                                                  'song_id' : 'category'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = df_train[['msno', 'song_id', 'target']]\n",
    "df_test = df_test[['msno', 'song_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lookup_tables(words):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param words: Input list of words\n",
    "    :return: A tuple of dicts.  The first dict....\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    word_counts = Counter(words)\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "\n",
    "    return vocab_to_int, int_to_vocab\n",
    "\n",
    "tot_user_ids = df_train['msno'].astype(object).unique()\n",
    "userid_to_int, int_to_userid = create_lookup_tables(tot_user_ids)\n",
    "tot_song_ids = df_train['song_id'].astype(object).unique()\n",
    "songid_to_int, int_to_songid = create_lookup_tables(tot_song_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train['user_no'] = [userid_to_int[user_id] for user_id in df_train['msno']]\n",
    "df_train['song_no'] = [songid_to_int[song_id] for song_id in df_train['song_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/test split\n"
     ]
    }
   ],
   "source": [
    "ratings = sqlCtx.createDataFrame(df_train)\n",
    "(training, test) = ratings.randomSplit([0.8, 0.2])\n",
    "print('train/test split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fit\n"
     ]
    }
   ],
   "source": [
    "# Build the recommendation model using ALS on the training data\n",
    "# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"user_no\", itemCol=\"song_no\", ratingCol=\"target\",\n",
    "          coldStartStrategy=\"drop\")\n",
    "model = als.fit(training)\n",
    "print('model fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict\n",
      "Root-mean-square error = 0.4763473531509393\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = model.transform(test)\n",
    "print('predict')\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"target\",\n",
    "                                predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rcmd generate finish\n"
     ]
    }
   ],
   "source": [
    "# Generate top 10 movie recommendations for each user\n",
    "userRecs = model.recommendForAllUsers(10)\n",
    "# Generate top 10 user recommendations for each movie\n",
    "movieRecs = model.recommendForAllItems(10)\n",
    "print('rcmd generate finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling pickle/cf_result_df_train_pred.\n",
      "pickle/cf_result_user_recs already present - Skipping pickling.\n",
      "pickle/cf_result_song_recs already present - Skipping pickling.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'pickle/cf_result_song_recs'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_pickle('pickle/cf_result_df_train_pred', predictions.toPandas())\n",
    "make_pickle('pickle/cf_result_user_recs', userRecs.toPandas())\n",
    "make_pickle('pickle/cf_result_song_recs', movieRecs.toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_userid_to_userno(user_id):\n",
    "    if user_id in userid_to_int:\n",
    "        return userid_to_int[user_id]\n",
    "    \n",
    "    return -1\n",
    "\n",
    "def convert_songid_to_songno(song_id):\n",
    "    if song_id in songid_to_int:\n",
    "        return songid_to_int[song_id]\n",
    "    \n",
    "    return -1\n",
    "\n",
    "df_test['user_no'] = [convert_userid_to_userno(user_id) for user_id in df_test['msno']]\n",
    "df_test['song_no'] = [convert_songid_to_songno(song_id) for song_id in df_test['song_id']]\n",
    "sp_df_test = sqlCtx.createDataFrame(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_predctions = model.transform(sp_df_test)\n",
    "df_test_pred = test_predctions.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling pickle/cf_result_df_test_pred.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'pickle/cf_result_df_test_pred'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_pickle('pickle/cf_result_df_test_pred', df_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msno</th>\n",
       "      <th>song_id</th>\n",
       "      <th>user_no</th>\n",
       "      <th>song_no</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UYsDRcm4UqNQVN+IrTfJX1MDsFCiGu/5EZT6MKb3B4o=</td>\n",
       "      <td>cy10N2j2sdY/X4BDUcMu2Iumfz7pV3tqE5iEaup2yGI=</td>\n",
       "      <td>6658</td>\n",
       "      <td>148</td>\n",
       "      <td>0.682439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>feaQJ9P/e04zHIiRjxpb9oqcrTrGO6Pei5iqswNAYTc=</td>\n",
       "      <td>cy10N2j2sdY/X4BDUcMu2Iumfz7pV3tqE5iEaup2yGI=</td>\n",
       "      <td>7554</td>\n",
       "      <td>148</td>\n",
       "      <td>0.512002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RYjTxubYo8PnNricR8ep8ZlR/I9jiHmkWnVywlNbPZ4=</td>\n",
       "      <td>cy10N2j2sdY/X4BDUcMu2Iumfz7pV3tqE5iEaup2yGI=</td>\n",
       "      <td>9900</td>\n",
       "      <td>148</td>\n",
       "      <td>0.302709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E47lQ6gQZfojmYN3St9RL5t5pE2tcwNbS7z/hVhR9AE=</td>\n",
       "      <td>cy10N2j2sdY/X4BDUcMu2Iumfz7pV3tqE5iEaup2yGI=</td>\n",
       "      <td>19553</td>\n",
       "      <td>148</td>\n",
       "      <td>0.408393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mdc75mqmDTdotiOVa9DVyNPQ+wivAJTczw1g9/rj+EU=</td>\n",
       "      <td>cy10N2j2sdY/X4BDUcMu2Iumfz7pV3tqE5iEaup2yGI=</td>\n",
       "      <td>23271</td>\n",
       "      <td>148</td>\n",
       "      <td>0.852764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           msno  \\\n",
       "0  UYsDRcm4UqNQVN+IrTfJX1MDsFCiGu/5EZT6MKb3B4o=   \n",
       "1  feaQJ9P/e04zHIiRjxpb9oqcrTrGO6Pei5iqswNAYTc=   \n",
       "2  RYjTxubYo8PnNricR8ep8ZlR/I9jiHmkWnVywlNbPZ4=   \n",
       "3  E47lQ6gQZfojmYN3St9RL5t5pE2tcwNbS7z/hVhR9AE=   \n",
       "4  mdc75mqmDTdotiOVa9DVyNPQ+wivAJTczw1g9/rj+EU=   \n",
       "\n",
       "                                        song_id  user_no  song_no  prediction  \n",
       "0  cy10N2j2sdY/X4BDUcMu2Iumfz7pV3tqE5iEaup2yGI=     6658      148    0.682439  \n",
       "1  cy10N2j2sdY/X4BDUcMu2Iumfz7pV3tqE5iEaup2yGI=     7554      148    0.512002  \n",
       "2  cy10N2j2sdY/X4BDUcMu2Iumfz7pV3tqE5iEaup2yGI=     9900      148    0.302709  \n",
       "3  cy10N2j2sdY/X4BDUcMu2Iumfz7pV3tqE5iEaup2yGI=    19553      148    0.408393  \n",
       "4  cy10N2j2sdY/X4BDUcMu2Iumfz7pV3tqE5iEaup2yGI=    23271      148    0.852764  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
