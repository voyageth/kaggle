{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import IPython.display\n",
    "from six.moves import cPickle as pickle\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def maybe_pickle(file_name, load_dataset, force=False):\n",
    "    pickle_file_name = \"pickle/2_\" + file_name + \".pickle\"\n",
    "    import os\n",
    "    if not os.path.exists(\"pickle\"):\n",
    "        os.makedirs(\"pickle\")\n",
    "        \n",
    "    if os.path.exists(pickle_file_name) and not force:\n",
    "        # You may override by setting force=True.\n",
    "        print('%s already present - Skipping pickling.' % pickle_file_name)\n",
    "    else:\n",
    "        print('Pickling %s.' % pickle_file_name)\n",
    "        dataset = load_dataset(None)\n",
    "        try:\n",
    "            with open(pickle_file_name, 'wb') as f:\n",
    "                pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "        except Exception as e:\n",
    "            print('Unable to save data to', file_name, ':', e)\n",
    "    \n",
    "    return pickle_file_name\n",
    "\n",
    "def load_data(file_name, force=False):\n",
    "    original_file_path = \"../input/\" + file_name + \".csv\"\n",
    "    pickle_file_name = maybe_pickle(file_name, lambda x: pd.read_csv(original_file_path), force)\n",
    "    \n",
    "    with open(pickle_file_name, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickle/2_biology.pickle already present - Skipping pickling.\n",
      "pickle/2_cooking.pickle already present - Skipping pickling.\n",
      "pickle/2_crypto.pickle already present - Skipping pickling.\n",
      "pickle/2_diy.pickle already present - Skipping pickling.\n",
      "pickle/2_robotics.pickle already present - Skipping pickling.\n",
      "pickle/2_travel.pickle already present - Skipping pickling.\n"
     ]
    }
   ],
   "source": [
    "biology = load_data(\"biology\")\n",
    "cooking = load_data(\"cooking\")\n",
    "crypto = load_data(\"crypto\")\n",
    "diy = load_data(\"diy\")\n",
    "robotics = load_data(\"robotics\")\n",
    "travel = load_data(\"travel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "punctuation_trans_table = str.maketrans({key: None for key in string.punctuation})\n",
    "html_tag_regex = re.compile('<.*?>')\n",
    "code_tag_regex = re.compile('<code>([^<]+)</code>', re.S)\n",
    "a_tag_regex = re.compile('<a href([^<]+)</a>', re.S)\n",
    "\n",
    "def cleaning_text(text):\n",
    "    original_text_length = len(text)\n",
    "    number_of_html_tag = len(re.findall(html_tag_regex, text))\n",
    "    number_of_code_fragments = len(re.findall(code_tag_regex, text))\n",
    "    number_of_a_href = len(re.findall(a_tag_regex, text))\n",
    "    \n",
    "    # convert to lowercase\n",
    "    text = text.lower()\n",
    "    # remove code fragment\n",
    "    text = re.sub(code_tag_regex, 'code_tag', text)\n",
    "    # remove html tags\n",
    "    text = re.sub(html_tag_regex, '', text)\n",
    "    # remove \\r, \\n\n",
    "    text = text.replace('\\n', ' ').replace('\\r', '')\n",
    "    # remove Punctuations\n",
    "    text = text.translate(punctuation_trans_table)\n",
    "    # split\n",
    "    words = word_tokenize(text)\n",
    "    # remove stop words\n",
    "    words = [word for word in words if word not in stopwords.words('english')]\n",
    "    # lemmatizing, stemming\n",
    "    #words = [wordnet_lemmatizer.lemmatize(word) for word in words]\n",
    "    #words = [stemmer.stem(word) for word in words]\n",
    "    # join\n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    number_of_cleaned_text_tokens = len(words)\n",
    "    cleaned_text_length = len(text)\n",
    "    return text, [original_text_length, number_of_html_tag, number_of_code_fragments, number_of_a_href, number_of_cleaned_text_tokens, cleaned_text_length]\n",
    "\n",
    "\n",
    "def cleaning(row):\n",
    "    row['title'], title_meta_list = cleaning_text(row['title'])\n",
    "    row['title_original_text_length'] = title_meta_list[0]\n",
    "    row['title_number_of_cleaned_text_tokens'] = title_meta_list[4]\n",
    "    row['title_cleaned_text_length'] = title_meta_list[5]\n",
    "    \n",
    "    row['content'], content_meta_list = cleaning_text(row['content'])\n",
    "    row['content_original_text_length'] = content_meta_list[0]\n",
    "    row['content_number_of_html_tag'] = content_meta_list[1]\n",
    "    row['content_number_of_code_fragments'] = content_meta_list[2]\n",
    "    row['content_number_of_a_href'] = content_meta_list[3]\n",
    "    row['content_number_of_cleaned_text_tokens'] = content_meta_list[4]\n",
    "    row['content_cleaned_text_length'] = content_meta_list[5]\n",
    "    \n",
    "    #row['cleaned_tags'], content_meta_list = cleaning_text(row['tags'])\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_cleaned_df(file_name, force=False):\n",
    "    original_file_path = \"../input/\" + file_name + \".csv\"\n",
    "    df = pd.read_csv(original_file_path)\n",
    "    print(\"total len : %d\" % len(df))\n",
    "    result_df = df.progress_apply(cleaning, axis=1)\n",
    "    \n",
    "    # feature scaling for meta columns\n",
    "    from sklearn import preprocessing\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    result_df['title_original_text_length'] = min_max_scaler.fit_transform(result_df['title_original_text_length'])\n",
    "    result_df['title_number_of_cleaned_text_tokens'] = min_max_scaler.fit_transform(result_df['title_number_of_cleaned_text_tokens'])\n",
    "    result_df['title_cleaned_text_length'] = min_max_scaler.fit_transform(result_df['title_cleaned_text_length'])\n",
    "    result_df['content_original_text_length'] = min_max_scaler.fit_transform(result_df['content_original_text_length'])\n",
    "    result_df['content_number_of_html_tag'] = min_max_scaler.fit_transform(result_df['content_number_of_html_tag'])\n",
    "    result_df['content_number_of_code_fragments'] = min_max_scaler.fit_transform(result_df['content_number_of_code_fragments'])\n",
    "    result_df['content_number_of_a_href'] = min_max_scaler.fit_transform(result_df['content_number_of_a_href'])\n",
    "    result_df['content_number_of_cleaned_text_tokens'] = min_max_scaler.fit_transform(result_df['content_number_of_cleaned_text_tokens'])\n",
    "    result_df['content_cleaned_text_length'] = min_max_scaler.fit_transform(result_df['content_cleaned_text_length'])\n",
    "    \n",
    "    return result_df\n",
    "    \n",
    "def maybe_pickle_cleaned_df(file_name, force=False):\n",
    "    pickle_file_name = maybe_pickle(file_name + \"_cleaned\", lambda x: load_cleaned_df(file_name), force)\n",
    "    \n",
    "    with open(pickle_file_name, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickle/2_biology_cleaned.pickle already present - Skipping pickling.\n",
      "pickle/2_cooking_cleaned.pickle already present - Skipping pickling.\n",
      "pickle/2_crypto_cleaned.pickle already present - Skipping pickling.\n",
      "pickle/2_diy_cleaned.pickle already present - Skipping pickling.\n",
      "pickle/2_robotics_cleaned.pickle already present - Skipping pickling.\n",
      "pickle/2_travel_cleaned.pickle already present - Skipping pickling.\n"
     ]
    }
   ],
   "source": [
    "biology_cleaned_df = maybe_pickle_cleaned_df('biology')\n",
    "cooking_cleaned_df = maybe_pickle_cleaned_df('cooking')\n",
    "crypto_cleaned_df = maybe_pickle_cleaned_df('crypto')\n",
    "diy_cleaned_df = maybe_pickle_cleaned_df('diy')\n",
    "robotics_cleaned_df = maybe_pickle_cleaned_df('robotics')\n",
    "travel_cleaned_df = maybe_pickle_cleaned_df('travel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87000\n"
     ]
    }
   ],
   "source": [
    "full_df = pd.concat([biology_cleaned_df, \n",
    "                     cooking_cleaned_df, \n",
    "                     crypto_cleaned_df, \n",
    "                     diy_cleaned_df, \n",
    "                     robotics_cleaned_df, \n",
    "                     travel_cleaned_df]).reset_index().drop('index', axis=1)\n",
    "print(len(full_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151721\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "stop_words = text.ENGLISH_STOP_WORDS\n",
    "\n",
    "full_df_vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "full_df_vectors = full_df_vectorizer.fit_transform((full_df['title'] + \" \" + full_df['content']).tolist())\n",
    "print(len(full_df_vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n"
     ]
    }
   ],
   "source": [
    "full_df_title_vectorizer = TfidfVectorizer(stop_words=stop_words, max_df=0.95, min_df=2,max_features=4000)\n",
    "full_df_title_vectors = full_df_title_vectorizer.fit_transform((full_df['title']).tolist())\n",
    "print(len(full_df_title_vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# code from https://buhrmann.github.io/tfidf-analysis.html\n",
    "def top_tfidf_feats(row, features, top_n=25):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids if row[i] > 0.3]\n",
    "    df = pd.DataFrame(top_feats, columns=['feature', 'tfidf'])\n",
    "    return df\n",
    "\n",
    "def top_feats_in_doc(Xtr, features, row_id, top_n=25):\n",
    "    ''' Top tfidf features in specific document (matrix row) '''\n",
    "    row = np.squeeze(Xtr[row_id].toarray())\n",
    "    return top_tfidf_feats(row, features, top_n)\n",
    "\n",
    "def predict_tags(vectors, vectorizer, index):\n",
    "    tfidf_df = top_feats_in_doc(vectors, vectorizer.get_feature_names(), index)\n",
    "    return ' '.join(tfidf_df['feature'])\n",
    "\n",
    "full_df_title_vectors2 = full_df_title_vectorizer.transform((full_df['title'] + ' ' + full_df['content']).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# return TP, FP, FN\n",
    "def compare_two_corpus(actual, predicted):\n",
    "    if len(actual) == 0:\n",
    "        if len(predicted) == 0:\n",
    "            return 0,0,0\n",
    "        else:\n",
    "            return 0,len(predicted.split()),0\n",
    "    else:\n",
    "        if len(predicted) == 0:\n",
    "            return 0,0,len(actual.split())\n",
    "        else:\n",
    "            actual_words = actual.split()\n",
    "            predicted_words = predicted.split()\n",
    "            tp = 0\n",
    "            fp = 0\n",
    "            fn = 0\n",
    "            for actual_word in actual_words:\n",
    "                if actual_word in predicted_words:\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    fn += 1\n",
    "            for predicted_word in predicted_words:\n",
    "                if predicted_word in actual_words:\n",
    "                    pass\n",
    "                else:\n",
    "                    fp += 1\n",
    "            \n",
    "            return tp, fp, fn\n",
    "\n",
    "def calculate_precision(tp, fp, fn):\n",
    "    return tp/float(tp + fp)\n",
    "\n",
    "\n",
    "def calculate_recall(tp, fp, fn):\n",
    "    return tp/float(tp + fn)\n",
    "        \n",
    "    \n",
    "def calculate_f1_score(tp, fp, fn, print_result=False):\n",
    "    if tp == 0:\n",
    "        if print_result:\n",
    "            print('tp : %d, fp : %d, fn : %d, precision : %f, recall : %f, f1_score : %f' % (tp, fp, fn, 0., 0., 0.))\n",
    "        return 0\n",
    "    precision = calculate_precision(tp, fp, fn)\n",
    "    recall = calculate_recall(tp, fp, fn)\n",
    "    f1_score = 2*precision*recall/(precision + recall)\n",
    "    if print_result:\n",
    "        print('tp : %d, fp : %d, fn : %d, precision : %f, recall : %f, f1_score : %f' % (tp, fp, fn, precision, recall, f1_score))\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rna biochemistry\n"
     ]
    }
   ],
   "source": [
    "print(full_df.loc[1, 'tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0row finished...\n",
      "1000row finished...\n",
      "2000row finished...\n",
      "3000row finished...\n",
      "4000row finished...\n",
      "5000row finished...\n",
      "6000row finished...\n",
      "7000row finished...\n",
      "8000row finished...\n",
      "9000row finished...\n",
      "10000row finished...\n",
      "11000row finished...\n",
      "12000row finished...\n",
      "13000row finished...\n",
      "14000row finished...\n",
      "15000row finished...\n",
      "16000row finished...\n",
      "17000row finished...\n",
      "18000row finished...\n",
      "19000row finished...\n",
      "20000row finished...\n",
      "21000row finished...\n",
      "22000row finished...\n",
      "23000row finished...\n",
      "24000row finished...\n",
      "25000row finished...\n",
      "26000row finished...\n",
      "27000row finished...\n",
      "28000row finished...\n",
      "29000row finished...\n",
      "30000row finished...\n",
      "31000row finished...\n",
      "32000row finished...\n",
      "33000row finished...\n",
      "34000row finished...\n",
      "35000row finished...\n",
      "36000row finished...\n",
      "37000row finished...\n",
      "38000row finished...\n",
      "39000row finished...\n",
      "40000row finished...\n",
      "41000row finished...\n",
      "42000row finished...\n",
      "43000row finished...\n",
      "44000row finished...\n",
      "45000row finished...\n",
      "46000row finished...\n",
      "47000row finished...\n",
      "48000row finished...\n",
      "49000row finished...\n",
      "50000row finished...\n",
      "51000row finished...\n",
      "52000row finished...\n",
      "53000row finished...\n",
      "54000row finished...\n",
      "55000row finished...\n",
      "56000row finished...\n",
      "57000row finished...\n",
      "58000row finished...\n",
      "59000row finished...\n",
      "60000row finished...\n",
      "61000row finished...\n",
      "62000row finished...\n",
      "63000row finished...\n",
      "64000row finished...\n",
      "65000row finished...\n",
      "66000row finished...\n",
      "67000row finished...\n",
      "68000row finished...\n",
      "69000row finished...\n",
      "70000row finished...\n",
      "71000row finished...\n",
      "72000row finished...\n",
      "73000row finished...\n",
      "74000row finished...\n",
      "75000row finished...\n",
      "76000row finished...\n",
      "77000row finished...\n",
      "78000row finished...\n",
      "79000row finished...\n",
      "80000row finished...\n",
      "81000row finished...\n",
      "82000row finished...\n",
      "83000row finished...\n",
      "84000row finished...\n",
      "85000row finished...\n",
      "86000row finished...\n"
     ]
    }
   ],
   "source": [
    "def predict_and_scoring(df, vector, vectorizer, index):\n",
    "    predicted_tags_by_title_content = predict_tags(vector, vectorizer, index)\n",
    "    actual_tags = df.loc[index, 'tags']\n",
    "    tp, fp, fn = compare_two_corpus(actual_tags, predicted_tags_by_title_content)\n",
    "    df.loc[index, 'predicted_tags'] = predicted_tags_by_title_content\n",
    "    df.loc[index, 'score_tp'] = tp\n",
    "    df.loc[index, 'score_fp'] = fp\n",
    "    df.loc[index, 'score_fn'] = fn\n",
    "    if i % 1000 == 0:\n",
    "        print(\"%drow finished...\" % i)\n",
    "\n",
    "for i in range(0,len(full_df)):\n",
    "    predict_and_scoring(full_df, full_df_title_vectors2, full_df_title_vectorizer, i)\n",
    "    #calculate_f1_score(tp, fp, fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.152494858297\n"
     ]
    }
   ],
   "source": [
    "print(calculate_f1_score(sum(full_df['score_tp']), sum(full_df['score_fp']), sum(full_df['score_fn'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickle/2_test.pickle already present - Skipping pickling.\n",
      "pickle/2_test_cleaned.pickle already present - Skipping pickling.\n"
     ]
    }
   ],
   "source": [
    "# for submit\n",
    "test = load_data(\"test\")\n",
    "test_cleaned_df = maybe_pickle_cleaned_df('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python35]",
   "language": "python",
   "name": "conda-env-python35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}